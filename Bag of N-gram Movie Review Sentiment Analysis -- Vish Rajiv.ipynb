{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please find the 3 correct/incorrect predictions of the best model at the end of this Jupyter notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-time loading of data from text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import spacy\n",
    "import string\n",
    "from os import listdir\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data loading code inspired from: https://machinelearningmastery.com/prepare-movie-review-data-sentiment-analysis/\n",
    "'''\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, data):\n",
    "    # walk through all files in the folder\n",
    "    i = 0\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load document\n",
    "        data.append(load_doc(path))\n",
    "        i += 1\n",
    "    print(\"Loaded {} files from {}\".format(i, directory))\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = '/Users/vrajiv/Desktop/NLP Class/lab3/aclImdb/train/'\n",
    "TEST_DATA_DIR = '/Users/vrajiv/Desktop/NLP Class/lab3/aclImdb/test/'\n",
    "TRAIN_VAL_SPLIT = 20000\n",
    "SEED = 1\n",
    "\n",
    "full_train_data = []\n",
    "\n",
    "num_pos = process_docs(TRAIN_DATA_DIR + 'pos', full_train_data)\n",
    "num_neg = process_docs(TRAIN_DATA_DIR + 'neg', full_train_data)\n",
    "\n",
    "full_train_labels = [0] * (num_pos + num_neg)\n",
    "for i in range(num_pos):\n",
    "    full_train_labels[i] = 1\n",
    "\n",
    "# randomly shuffle data \n",
    "# do it for labels as well, so they get shuffled the same way\n",
    "random.Random(SEED).shuffle(full_train_data)\n",
    "random.Random(SEED).shuffle(full_train_labels)\n",
    "\n",
    "# Split train data into actual train and validation sets\n",
    "train_data = full_train_data[:TRAIN_VAL_SPLIT]\n",
    "val_data = full_train_data[TRAIN_VAL_SPLIT:]\n",
    "\n",
    "# Same for labels\n",
    "train_labels = full_train_labels[:TRAIN_VAL_SPLIT]\n",
    "val_labels = full_train_labels[TRAIN_VAL_SPLIT:]\n",
    "\n",
    "print(\"Total length of train data: {}\".format(len(train_data)))\n",
    "print(\"Total length of validation data: {}\".format(len(val_data)))\n",
    "\n",
    "\n",
    "###\n",
    "### TEST DATA \n",
    "###\n",
    "\n",
    "test_data = []\n",
    "\n",
    "num_pos = process_docs(TEST_DATA_DIR + 'pos', test_data)\n",
    "num_neg = process_docs(TEST_DATA_DIR + 'neg', test_data)\n",
    "\n",
    "test_labels = [0] * (num_pos + num_neg)\n",
    "for i in range(num_pos):\n",
    "    test_labels[i] = 1\n",
    "\n",
    "# randomly shuffle data\n",
    "random.Random(SEED).shuffle(test_data)\n",
    "random.Random(SEED).shuffle(test_labels)\n",
    "print(\"Total length of test data: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that labels (sentiment) and reviews match in train data. \n",
    "def verify_match(data, labels):\n",
    "    rand_seed = random.randint(1, 101)\n",
    "    print(random.Random(rand_seed).choice(data))\n",
    "    print(random.Random(rand_seed).choice(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_match(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_match(val_data, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing train/val/test as lists -- pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running the above code!\n",
    "\n",
    "# with open('movie_review_train_data.p', 'wb') as f:\n",
    "#     pkl.dump(train_data, f)\n",
    "    \n",
    "# with open('movie_review_train_labels.p', 'wb') as f:\n",
    "#     pkl.dump(train_labels, f)\n",
    "    \n",
    "# with open('movie_review_val_data.p', 'wb') as f:\n",
    "#     pkl.dump(val_data, f)\n",
    "    \n",
    "# with open('movie_review_val_labels.p', 'wb') as f:\n",
    "#     pkl.dump(val_labels, f)\n",
    "    \n",
    "# with open('movie_review_test_data.p', 'wb') as f:\n",
    "#     pkl.dump(test_data, f)\n",
    "    \n",
    "# with open('movie_review_test_labels.p', 'wb') as f:\n",
    "#     pkl.dump(test_labels, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching train/val/test data -- pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pkl.load(open(\"movie_review_train_data.p\", \"rb\"))\n",
    "train_labels = pkl.load(open(\"movie_review_train_labels.p\", \"rb\"))\n",
    "val_data = pkl.load(open(\"movie_review_val_data.p\", \"rb\"))\n",
    "val_labels = pkl.load(open(\"movie_review_val_labels.p\", \"rb\"))\n",
    "test_data = pkl.load(open(\"movie_review_test_data.p\", \"rb\"))\n",
    "test_labels = pkl.load(open(\"movie_review_test_labels.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This definitely is NOT the intellectual film with profound mission, so I really don't think there is too much not to understand to in case you aren't Czech.<br /><br />It's just a comedy. The humor is simple, pretty funny and sometimes, maybe, little morbid. Some actors and characters are very similar to Samotári (2000) (Jirí Machácek, Ivan Trojan, Vladimír Dlouhý) so the authors are. But it doesn't matter, the genre is really different and these two films shouldn't be compared in this way. Jedna ruka netleská won't try to give you a lesson, it will try to make you laugh and there is some chance it will succeed.<br /><br />Not bad film, not the ingenious one, but I enjoyed it. Some scenes are truly worth seeing.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization (standard scheme from lab + 2nd scheme: remove stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'looking', 'buying', 'u.k.', 'startup', '1', 'billion']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation as well as stop words (for 2nd tokenization scheme\n",
    "def tokenize(sent):\n",
    "    tokens = tokenizer(sent)\n",
    "    return [token.text.lower() for token in tokens if (token.text not in punctuations and token.text not in STOP_WORDS)]\n",
    "\n",
    "# Example\n",
    "tokens = tokenize(u'Apple is looking at buying U.K. startup for $1 billion')\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing datasets takes 30-40 min, so only uncomment if need to re-tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing test data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset \n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens = tokenize(sample)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens\n",
    "\n",
    "# # val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "# pkl.dump(val_data_tokens, open(\"movie_review_val_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# # test set tokens\n",
    "# print (\"Tokenizing test data\")\n",
    "# test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "# pkl.dump(test_data_tokens, open(\"movie_review_test_data_tokens.p\", \"wb\"))\n",
    "\n",
    "# # train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "# pkl.dump(train_data_tokens, open(\"movie_review_train_data_tokens.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens, open(\"movie_review_all_train_tokens.p\", \"wb\"))\n",
    "\n",
    "###\n",
    "### REMOVING STOP WORDS\n",
    "###\n",
    "\n",
    "# # # val set tokens\n",
    "# print (\"Tokenizing val data\")\n",
    "# val_data_tokens, _ = tokenize_dataset(val_data)\n",
    "# pkl.dump(val_data_tokens, open(\"movie_review_val_data_tokens_stop_words.p\", \"wb\"))\n",
    "\n",
    "# # # test set tokens\n",
    "# print (\"Tokenizing test data\")\n",
    "# test_data_tokens, _ = tokenize_dataset(test_data)\n",
    "# pkl.dump(test_data_tokens, open(\"movie_review_test_data_tokens_stop_words.p\", \"wb\"))\n",
    "\n",
    "# # # train set tokens\n",
    "# print (\"Tokenizing train data\")\n",
    "# train_data_tokens, all_train_tokens = tokenize_dataset(train_data)\n",
    "# pkl.dump(train_data_tokens, open(\"movie_review_train_data_tokens_stop_words.p\", \"wb\"))\n",
    "# pkl.dump(all_train_tokens, open(\"movie_review_all_train_tokens_stop_words.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading tokenized datasets -- train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "Total number of tokens in train dataset is 2519291\n"
     ]
    }
   ],
   "source": [
    "# Uncomment for standard tokenization scheme from lab!\n",
    "\n",
    "# train_data_tokens = pkl.load(open(\"movie_review_train_data_tokens.p\", \"rb\"))\n",
    "# all_train_tokens = pkl.load(open(\"movie_review_all_train_tokens.p\", \"rb\"))\n",
    "# val_data_tokens = pkl.load(open(\"movie_review_val_data_tokens.p\", \"rb\"))\n",
    "# test_data_tokens = pkl.load(open(\"movie_review_test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "### STOP WORDS -- Uncomment for tokenization scheme 2\n",
    "\n",
    "train_data_tokens = pkl.load(open(\"movie_review_train_data_tokens_stop_words.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"movie_review_all_train_tokens_stop_words.p\", \"rb\"))\n",
    "val_data_tokens = pkl.load(open(\"movie_review_val_data_tokens_stop_words.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"movie_review_test_data_tokens_stop_words.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build vocabulary from most common tokens\n",
    "#### Includes two functions to generate n-grams only and n-grams inclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# save index 0 for unk and 1 for pad\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "'''\n",
    "Inspired from: http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "'''\n",
    "def generate_bag_ngrams(n, tokens):\n",
    "    return zip(*[tokens[i:] for i in range(n)])\n",
    "\n",
    "def generate_list_ngrams_inclusive(n, tokens):\n",
    "    ret = []\n",
    "    for i in range(n):\n",
    "        ret = ret + list(generate_bag_ngrams(i+1, tokens))\n",
    "    return ret\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(VOCAB_SIZE))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"One-hot encoding\"\n",
    "#### Represent each movie review as a vector of indices in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining PyTorch Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 200\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MovieReviewDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of movie review tokens \n",
    "        @param target_list: list of movie review targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def moviereview_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Bag-of-Ngrams model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # Apply the linear function to get our logit (real numbers)\n",
    "        logit = self.linear(out.float())\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "def train_model(model,\n",
    "              lr = 0.005, \n",
    "              num_epochs = 4, \n",
    "              criterion = nn.CrossEntropyLoss()):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) # wd: linear\n",
    "    \n",
    "    # Uncomment for linear annealing of learning rate\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "    losses = []\n",
    "    xs = 0\n",
    "    val_accs = []\n",
    "    for epoch in range(num_epochs):\n",
    "#         outside_loss = 0\n",
    "        for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "            model.train()\n",
    "            data_batch, length_batch, label_batch = data, lengths, labels\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data_batch, length_batch)\n",
    "            loss = criterion(outputs, label_batch)\n",
    "            outside_loss = loss\n",
    "            losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # validate every 100 iterations\n",
    "            if i > 0 and i % 100 == 0:\n",
    "                # validate\n",
    "                val_acc = test_model(val_loader, model)\n",
    "                xs = xs + 100\n",
    "                val_accs.append(val_acc)\n",
    "                print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                           epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n",
    "#         scheduler.step(outside_loss)\n",
    "    return losses, xs, val_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full model run in one script (for quick hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 27674 ; token ('vendetta',)\n",
      "Token ('vendetta',); token id 27674\n",
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n",
      "tensor([[   10,     3,    16,  ...,     0,     0,     0],\n",
      "        [ 6777,  7221,   616,  ...,     0,     0,     0],\n",
      "        [    2,   165,    58,  ..., 21625,  3324,  4639],\n",
      "        ...,\n",
      "        [    2,     8,   939,  ...,     1,     1,  4653],\n",
      "        [  299, 10717,    25,  ...,     1,     1,     1],\n",
      "        [   44,  5421,   536,  ...,  4549,   439,  4502]])\n",
      "tensor([183, 183, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 198, 200,\n",
      "        165, 200, 200, 200, 200, 200, 105, 200, 200, 200, 200, 200, 200, 200,\n",
      "        200, 200, 200, 200])\n",
      "tensor([0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "NGRAM_SIZE = 3\n",
    "EMBED_DIM = 300\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_EPOCHS = 4\n",
    "\n",
    "# Building the vocabulary for n-gram only\n",
    "# Uncomment for n-gram only\n",
    "\n",
    "# token2id, id2token = build_vocab(generate_bag_ngrams(NGRAM_SIZE, all_train_tokens))\n",
    "\n",
    "# Building the vocabulary for n-gram inclusive\n",
    "# Uncomment for n-gram inclusive\n",
    "\n",
    "token2id, id2token = build_vocab(generate_list_ngrams_inclusive(NGRAM_SIZE, all_train_tokens))\n",
    "\n",
    "# Check the dictionary by loading a random token from it\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))\n",
    "\n",
    "# Representing each movie review in train/val/test as a bag-of-ngrams\n",
    "# Uncomment for n-gram only. \n",
    "\n",
    "# ngram_train_data_tokens = [list(generate_bag_ngrams(NGRAM_SIZE, review)) for review in train_data_tokens]\n",
    "# ngram_val_data_tokens = [list(generate_bag_ngrams(NGRAM_SIZE, review)) for review in val_data_tokens]\n",
    "# ngram_test_data_tokens = [list(generate_bag_ngrams(NGRAM_SIZE, review)) for review in test_data_tokens]\n",
    "\n",
    "# Uncomment for n-gram inclusive (<= n).\n",
    "ngram_train_data_tokens = [generate_list_ngrams_inclusive(NGRAM_SIZE, review) for review in train_data_tokens]\n",
    "ngram_val_data_tokens = [generate_list_ngrams_inclusive(NGRAM_SIZE, review) for review in val_data_tokens]\n",
    "ngram_test_data_tokens = [generate_list_ngrams_inclusive(NGRAM_SIZE, review) for review in test_data_tokens]\n",
    "\n",
    "# \"One-hot encoding\"\n",
    "train_data_indices = token2index_dataset(ngram_train_data_tokens)\n",
    "val_data_indices = token2index_dataset(ngram_val_data_tokens)\n",
    "test_data_indices = token2index_dataset(ngram_test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = MovieReviewDataset(train_data_indices, train_labels)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE, \n",
    "                                           collate_fn=moviereview_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = MovieReviewDataset(val_data_indices, val_labels)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviereview_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = MovieReviewDataset(test_data_indices, test_labels)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=moviereview_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "    print (data)\n",
    "    print(lengths)\n",
    "    print (labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/4], Step: [101/625], Validation Acc: 76.84\n",
      "Epoch: [1/4], Step: [201/625], Validation Acc: 83.64\n",
      "Epoch: [1/4], Step: [301/625], Validation Acc: 87.2\n",
      "Epoch: [1/4], Step: [401/625], Validation Acc: 88.02\n",
      "Epoch: [1/4], Step: [501/625], Validation Acc: 88.66\n",
      "Epoch: [1/4], Step: [601/625], Validation Acc: 88.44\n",
      "Epoch: [2/4], Step: [101/625], Validation Acc: 89.62\n",
      "Epoch: [2/4], Step: [201/625], Validation Acc: 89.7\n",
      "Epoch: [2/4], Step: [301/625], Validation Acc: 88.94\n",
      "Epoch: [2/4], Step: [401/625], Validation Acc: 89.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-cede39abd4e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           criterion = nn.CrossEntropyLoss())\n\u001b[0m",
      "\u001b[0;32m<ipython-input-167-be36f0b50d24>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, lr, num_epochs, criterion)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;31m# validate every 100 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpclass/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BagOfWords(len(id2token), EMBED_DIM)\n",
    "\n",
    "losses, xs, val_accs = train_model(model,\n",
    "          lr = LEARNING_RATE, \n",
    "          num_epochs = NUM_EPOCHS, \n",
    "          criterion = nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting training (loss) and validation (accuracy) curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Val Acc')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(4, 3))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "loss_avg_vals = []\n",
    "for i in range(0, len(losses)-100, 100):\n",
    "    s = 0\n",
    "    avg = 0\n",
    "    for j in range(i, i+100):\n",
    "        s += losses[j]\n",
    "    avg = s/100.0\n",
    "    loss_avg_vals.append(avg)\n",
    "    \n",
    "plt.suptitle(\"Best model\")\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(len(loss_avg_vals)), loss_avg_vals)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Average Train Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(0, xs, 100), val_accs)\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Val Acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding 3 correct and 3 incorrect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  0\n",
      "correct:  1\n",
      "correct:  1\n",
      "incorrect:  0\n",
      "incorrect:  1\n",
      "incorrect:  1\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "eval_dataset = MovieReviewDataset(val_data_indices, val_labels)\n",
    "eval_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                          batch_size=1,\n",
    "                                          collate_fn=moviereview_collate_func,\n",
    "                                          shuffle=False)\n",
    "\n",
    "i = 0\n",
    "incorrect_data = []\n",
    "correct_data = []\n",
    "corr_count = 0\n",
    "incorr_count = 0\n",
    "for data, lengths, labels in eval_loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        if (predicted.squeeze().item() == labels.squeeze().item() and corr_count <= 2):\n",
    "            corr_count += 1\n",
    "            correct_data.append(val_data[i])\n",
    "            print(\"correct: \", labels.squeeze().item())\n",
    "        elif (predicted.squeeze().item() != labels.squeeze().item() and incorr_count <= 2):\n",
    "            incorr_count += 1\n",
    "            incorrect_data.append(val_data[i])\n",
    "            print(\"incorrect: \", labels.squeeze().item())\n",
    "        i += 1\n",
    "        \n",
    "        if corr_count == 3 and incorr_count == 3:\n",
    "            break       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews classified correctly: 0, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This series has its ups and occasional downs, and the latter is the case, here. There's an agreeable amount of spatter, with an inventive implementation of the Baby Cart's weapons, but the editing film is a seriously disjointed, the film-making itself rougher than usual. At times, the action slows to a crawl as the camera follows the wordless wanderings of the \"cub,\" who nearly gets lost early on. All in all, disappointment.<br /><br />That said, there's a spaghetti eastern quality to the music and action that may win the approval of dedicated viewers. This installment spends much of its time following the minor misadventures of the little boy, who begins to stare into the abyss of death his father opened for him.\n",
      "\n",
      "Following the brilliant \"Goyôkiba\" (aka. \"Hanzo The Razor - Sword Of Justice\", 1972) and its excellent (and even sleazier) sequel \"Goyôkiba: Kamisori Hanzô jigoku zeme\" (aka. \"Razor 2: The Snare\", 1973), this \"Goyôkiba: Oni no Hanzô yawahada koban\" aka. \"Razor 3: Who's Got The Gold\" is the third, and sadly final installment to the awesome saga about the incorruptible Samurai-constable Hanzo 'The Razor' Ittami (brilliantly played by the great Shintarô Katsu), who fights corruption with his fighting expertise as well as his enormous sexual powers. As a big fan of 70s exploitation cinema made in Nippon, \"Sword Of Justice\" became an instant favorite of mine, and I was therefore more than eager to find the sequels, and full of anticipation when I finally stumbled over them recently. While this third \"Hanzo\" film is just not quite as brilliant as its predecessors it is definitely another great piece of cult-cinema that no lover of Japanese exploitation cinema can afford to miss. \"Who's Got The Gold\" is a bit tamer than the two foregoing Hanzo films, but it is just as brilliantly comical and crudely humorous, and immediately starts out fabulously odd: The film begins, when Hanzo's two assistants see a female ghost when fishing. Having always wanted to sleep with a ghost, Hanzo insists that his assistants lead him to the site of the occurrence... If that is not a promising beginning for an awesome film experience, I don't know what is. Shintaro Katsu, one of my personal favorite actors, is once again brilliant in the role of Hanzo, a role that seems to have been written specifically for him. Katsu IS Hanzo, the obstinate and fearless constable, who hates corruption and deliberately insults his superiors, and whose unique interrogation techniques include raping female suspects. The interrogated women than immediately fall for him, due to his sexual powers and enormous penis, which he trains in a rather grotesque routine ritual. I will not give away more about the plot in \"Who's Got The Gold\", but I can assure that it is as cool as it sounds. The supporting performances are also very good, and, as in the predecessors, there are plenty of hilariously eccentric characters. This is sadly the last film in the awesomely sleazy 'Hanzo' series. If they had made 20 sequels more, I would have happily watched them all! The entire Hanzo series is brilliant, and while this third part is a bit inferior compared to its predecessors, it is definitely a must-see for all lovers of cult-cinema! Oh how I wish they had made more sequels!\n",
      "\n",
      "I have no idea how a Texan (the director, Douglas McGrath) and the American actress Gwyneth Paltrow ever pulled this off but seeing this again will remind you what all the fuss about Ms. Paltrow was in the first place! I had long since gone off the woman and still feel she is rather dull in her Oscar-winning \"Shakespeare In Love\" performance but she gets all the beats right here--she is nigh on perfect as Emma Woodhouse. She may have won her Oscar for Shakespeare but she should be remembered for this.<br /><br />Of course, she's surrounded by a great supporting cast including Toni Collette, Greta Scacchi, Juliette Stevenson et al...Jeremy Northam is very appealing as the love interest, even if the script wallows a bit in his declaration of love to Paltrow (in the process, allowing all of the tension to drain out of their relationship); several years on, Ewan's hair is a little easier to take than it was in '96 and, personally, I find puckish Alan Cumming a grating presence in anything nowadays. But the standout is, without a doubt, Sophie Thompson (sister of Emma Thompson, daughter of Phyllida Law) as Miss Bates; what this version needs is a scene where Emma reconciles with Miss Bates, as she is the character to whose fate we are drawn. The film is worth watching (again even) for her performance alone.<br /><br />All in all, this has aged wonderfully with charm to spare and more than enough subtlety to sort out the British class system. Well worth a rental (because its unlikely that Paltrow will ever be this good again--but we'll always have Emma).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(correct_data)):\n",
    "    print(correct_data[i] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews classified incorrectly : correct labels are 0, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I saw this movie at the AFI Dallas festival. Most of the audience, including my wife, enjoyed this comedy-drama, but I didn't. It stars Lucas Haas (Brick, Alpha Dog), Molly Parker (Kissed, The Five Senses, Hollywoodland) and Adam Scott (First Snow, Art School Confidential). The director is Matt Bissonnette, who's married to Molly Parker. All three actors do a fine job in this movie about 3 friends, the marriage of two of them and infidelity involving the third. It all takes place at a lake house and it looks wonderful. The film wants to treat its subject as a comedy first and then a drama, and I thought it needed to be the other way around.\n",
      "\n",
      "As Most Off You Might off Seen Star Wars: Return Off The Jedi You May Knows Its A Good Movie But As You Might Have Seen On Video They M|might have a party At The end And They Just Probably End The Movie with the party with no a spirits or anything But on the original one (Live TV) When they are Partying But before i say more when Ben obi-wan dies in the Imperial Ship Or Death Star They Saw him Disappear And Yoda Dies From Either Old Age Or Internal Illness But because Luke killed Darth Vader (Real Name: Anakin Skywalker) When They All Are Partying At The end when Luke Or Someone Stops the Spirits Off Ben And Yoda Stands Starring At Him And Smiling While Another Spirit Appears Is its Darth Vader but not as A Sith As The Old Usual Selve off Him And Started Smiling with Ben And Yoda I reckon That made the movie ending a little bit interesting But the Producers or anyone should off made a spirit off Padme And Mace Windu And Other Jedis that got killed with Younglings Under There Arms in the back ground\n",
      "\n",
      "An excellent example of the spectacular Busby Berkeley musicals produced in the early 1930's. Audiences must've been very surprised to see James Cagney in this type of vehicle. Quite a contrast from his \"Public Enemy\" 2 years earlier. Cagney does add spark & interest to a rather routine tired out formulated storyline & plot. But the highlight of the movie is the 3 elaborate production numbers back to back. First with the conservative \"Honeymoon Hotel\" number,then followed by the very spectacularly eye dazzling \"By A Waterfall\" sequence,followed by the closing \"Shanghai Lil\" sequence, Cagney only participates in the last number hoofing it up on top of a bar counter with Ruby Keeler. The \"Shanghai Lil\" number with Cagney is excellent but a bit of a comedown & anti climactic after the more exciting & incredibly mind boggling \"By A Waterfall\" choreography.If I was the director I would've inserted the \"Shanghai Lil\" number in the middle & close with \"By A Waterfall\",which blows the other 2 numbers out of the water so to speak & in my view the best of the 3 numbers. The 3 production numbers are the frosting on the cake & James Cagney's performance is added decoration to the cake. An outstanding musical achievement,a 4 star movie, the ultimate musical,well worth watching,you won't be disappointed!!!!!!!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(incorrect_data)):\n",
    "    print(incorrect_data[i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 4 epochs\n",
      "Val Acc 90.12\n",
      "Test Acc 87.724\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(NUM_EPOCHS))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
